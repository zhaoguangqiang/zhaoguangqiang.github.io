---
layout:	post
title:	"use k8s"
date:	2019-08-27
tags:	["kubernetes"]
image:	""
---
https://www.zeusro.tech/2019/01/25/kubernetes-experience/
[调研](https://blog.csdn.net/watermelonbig/article/details/79346524)
[k8s调度](http://dockone.io/article/2885)

use k8s
===

升级及回滚
---
#### 回滚相关操作
```
kubectl describe deployments  #查询详细信息，获取升级进度
kubectl rollout pause deployment/DEPLOYMENT_NAME #暂停升级
kubectl rollout resume deployment/DEPLOYMENT_NAME  #继续升级
kubectl rollout undo deployment/DEPLOYMENT_NAME  #升级回滚
kubectl rollout history deployment/DEPLOYMENT_NAME #查看升级历史
```
Deployment.spec.revisionHistoryLimit来限制最大保留的revision number

验证粘性会话
---

#### 基于 TCP 层的粘性会话

 * service配置sessionAffinity:ClientIP可以通过客户端ip匹配后端的pod

 * 使用方法: Service.spec.sessionAffinity:ClusterIP

#### 为什么基于ingress反向代理后, Service.spec.sessionAffinity不生效?

ingress-nginx访问后端的pod，虽然需要借助service的iptable的路由查看后端存在哪些pod，但是最终访问服务时是绕过service层直接操作后端pod的（这块之前理解也有误，一直以为ingress转发到了service，service发送至了pod）,所以service的sessionAffinity并不生效。

#### 基于 HTTP 层的粘性会话 **

事实上ingress-nginx也有粘滞连接的方法，在ingress的yaml配置中的Ingress.metadata.annotations注释中加入如下内容，就会根据不同的pod生成不同的hash来粘滞

```
	nginx.ingress.kubernetes.io/affinity: "cookie"
	nginx.ingress.kubernetes.io/session-cookie-expires: "172800"
	nginx.ingress.kubernetes.io/session-cookie-max-age: "172800"
```

* 相关资料可参考：https://kubernetes.github.io/ingress-nginx/examples/affinity/cookie/ *

自动扩容
---

增加autoscale.yaml
```
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
	name: ngx-dep
spec:
	maxReplicas: 28
	minReplicas: 1
	scaleTargetRef:
		apiVersion: extensions/v1beta1
		kind: Deployment
		name: ngx-dep
	targetCPUUtilizationPercentage: 80

$ kubectl apply -f autoscale.yaml
```

k8s 功能性操作
===

重置节点
---

### master上隔离并删除节点
* SchedulingDisabled,确保新的容器不会调度到该节点
kubectl cordon <node-name>
kubectl drain <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-local-data

kubectl delete node <node-name>

#### client上重置节点并重新加入master
sudo kubeadm reset
sudo kubeadm join 172.23.127.113:6443 --token atg34o.2nwr0dn6ttojbt4h --discovery-token-ca-cert-hash sha256:3d236eac9a79894db27ed38cef6022e7051c5df54384a3f3d9740ab57fb92e35 

批量删除Evicted Pod
---
kubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod

注意事项
===

不要向容器中存储日志
---
disk space属于不可压缩资源
如果日志量过大，会导致DiskPressure

在ingress映射path后，不同svc不要使用相同的path
---
* 在测试过程中碰到了这种问题，通过svc的ip访问不同svc，发现无法提供服务，后关闭ingress该问题就消失了。
* 原因：ingress把path指向了某个label的svc
* 因此，在生产过程中一定不要在不同svc上使用相同path，否则某些svc无法提供服务

升级方案
===
[升级方案](https://blog.container-solutions.com/kubernetes-deployment-strategies)
[github](https://github.com/ContainerSolutions/k8s-deployment-strategies)

k8s默认提供了两种升级方案:
---
* kubectl explain Deployment.spec.strategy.type: "Recreate" or "RollingUpdate"
* Recreate: 在Terminating后重新创建pod,会出现服务间断的问题,仅适用于开发过程中使用

#### RollingUpdate 

* RollingUpdate: 滚动更新,保证不间断提供服务;但会存在新老版本共存的窗口期,期望值始终要等于 max pod, 与max available pod
* ReplicaSet和rollout history的内在关系
* 滚动更新的流程
* rollout pause和resume
```text
max Surge = 2
max Unavailable = 1

6 = current desired(4) + max Surge
3 = current desired(4) - max Unavailable

NAME             DESIRED   CURRENT   READY   AGE
rollupdate-new   0         0         0       36m
rollupdate-old   4         4         4       34m
rollupdate-new   0         0         0       36m

new desired = 6 - old desired = 2
rollupdate-new   2         0         0       36m

old desired = 3 - new ready = 3
rollupdate-old   3         4         4       34m
rollupdate-old   3         4         4       34m
rollupdate-new   2         0         0       36m
rollupdate-new   2         1         0       36m
rollupdate-new   2         2         0       36m
rollupdate-old   3         3         3       34m


new desired = 6 - old desired = 3
rollupdate-new   3         2         0       36m
rollupdate-new   3         2         0       36m
rollupdate-new   3         3         0       36m
rollupdate-new   3         3         1       37m


old desired = 3 - new ready = 2
rollupdate-old   2         3         3       34m

new desired = 6 - old desired = 4
rollupdate-new   4         3         1       37m
rollupdate-old   2         3         3       34m
rollupdate-new   4         3         1       37m
rollupdate-old   2         2         2       34m
rollupdate-new   4         4         1       37m
rollupdate-new   4         4         2       37m


old desired = 3 - new ready = 1
rollupdate-old   1         2         2       35m
rollupdate-old   1         2         2       35m
rollupdate-old   1         1         1       35m
rollupdate-new   4         4         3       38m
rollupdate-old   0         1         1       35m
rollupdate-old   0         1         1       35m
rollupdate-old   0         0         0       35m
rollupdate-new   4         4         4       38m
```
特殊的配置:
maxSurge: 0 //保证如果内存不够时先退出
maxUnavailable: 1 //保证one by one deploy

k8s默认
---
#### blue/green

2个版本同时发布，最终切换到新版本

* 实时部署/回滚
* 避免版本问题，因为一次更改是整个应用的改变
* 需要两倍的资源
* 在发布到生产之前，应该对整个应用进行适当的测试

#### canary

让部分用户访问到新版本应用

* 部分用户获取新版本
* 方便错误和性能监控
* 快速回滚
* 发布较慢

#### A/B

基于统计信息而非部署策略来制定业务决策的技术,通过Http header,cache等内容来控制流量的精准分配，需要借助其他工具Istio

* 几个版本并行运行
* 完全控制流量分配
* 特定的一个访问错误难以排查，需要分布式跟踪
* Kubernetes 没有直接的支持，需要其他额外的工具


